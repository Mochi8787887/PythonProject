------------------------------------------優化深度學習模型的技巧（上）------------------------------------------
文章參考: https://ithelp.ithome.com.tw/articles/10203542

一. Batch & Mini batch
深度學習每一次參數的更新所需要損失函數並不是由一個資料點而來的，而是由一組數據加權得到的，稱為 batch。 

1. Full Batch: batch size 為樣本總數。如果數據集較小，可以使用 Full batch learning 的形式
優點：具有朝向最小值的直線軌跡，並且如果損失函數是凸的（convex）則保證在理論上收斂到全局最小值，並且如果損失函數不是凸的則保證局部最小值。
缺點：如果數據集很龐大，速度可能會很慢。
2. Mini batch：解決上述方法的缺點，提高學習效率，將訓練集分成很多批（batch），對每一批計算誤差並更新參數，是深度學習中很常見的學習方式。

二. Stochastic Gradient Descent (SGD)
隨機梯度下降（SGD）在每個樣本上執行參數更新，學習過程增加了更多的噪音，有助於改善泛化錯誤，但會增加運行時間。

三. Momentum （動量）
與物理慣性有關，基本思路是為了尋找最佳解而加入了「慣性」的影響。
好處是振盪的幅度變小且縮短了到達一定地點的時間，另外有可能透過慣性來讓梯度下降跳出局部最小值（local minimum）。


------------------------------------------優化深度學習模型的技巧（中）------------------------------------------
文章參考: https://ithelp.ithome.com.tw/articles/10204032

「學習率（Learning Rate）」這個參數，Learning Rate 掌握模型的學習進度，如何調整學習率是訓練出好模型的關鍵要素。
如果學習率太小，代表對神經網絡進行非常小的權重更新，會使其訓練變非常緩慢；然後學習率太大，可能導致無法收斂。

一. AdaGrad：
針對每個參數客制化的值，對學習率進行約束，依照梯度去調整學習率。
優點:是能加快訓練速度，在前期梯度較小時（較平坦）能夠放大梯度，後期梯度較大時（陡峭）能約束梯度
缺點:在訓練中後段時有可能梯度趨近於 0，而過早结束學習過程。

二. RMSProp：
Geoff Hinton 所提出，可改善 AdaGrad 的缺點。
MSProp 比 AdaGrad 多了一個衰減系統，它會聯繫之前的每一次梯度變化情況來更新學習率，緩解 Adagrad 學習率下降過快的問題。

三. Adam(待研究)：
——直覺來說 Adam 是 AdaGrad 跟 Momentum 的融合
——Adam 是 rmsprop 與 momentum 的組合，從參數去看，也能看到 rmsprop 的 alpha 影子。
優點主要在於它有做偏置校正，使每次迭代學習率都有個確定範圍，讓參數的更新較為平穩。


------------------------------------------優化深度學習模型的技巧（下）------------------------------------------
文章參考: https://ithelp.ithome.com.tw/articles/10204106

Normalization 的好處：在具有統一規格的資料下，Machine Learning 能更容易學到資料中的規律

一. Batch Normalization的優點
它的提出是為了克服深度神經網絡難以訓練的問題。使用 Batch Normalization 優點在於：
1. 快速學習（能增加學習率）
2. 不會過度依賴預設值（不會對預設值產生過度反應）
3. 控制過度學習（減少 Dropout 等必要性）

二. Batch Normalization 的運作
進行學習時以 mini-batch 為單位，依照各個 mini-batch 來進行正規化。
為了增加神經網絡的穩定性，它透過減去 batch 的均值並除以 batch 的標準差（standard deviation）來正規化前面激活層（activation）的輸出。

三. Batch Normalization 的使用時機
1. 遇到收歛速度很慢，或梯度爆炸等無法訓練的狀況時可以嘗試
2. 在一般使用情况下也可以加入，用來加快訓練速度，提高模型效能。
